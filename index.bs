<pre class='metadata'>
Title: Implementation guidelines for NDE alignment of cultural heritage data management and publication infrastructure
Shortname: implementation-guidelines
Status: LS
Markup Shorthands: css yes, markdown yes
URL: https://netwerk-digitaal-erfgoed.github.io/cm-implementation-guidelines/
Editor:
    Sjors de Valk, Dutch Digital Heritage Network https://www.netwerkdigitaalerfgoed.nl/en, sjors@sjorsdevalk.nl
    Ivo Zandhuis, Dutch Digital Heritage Network https://www.netwerkdigitaalerfgoed.nl/en, ivo@zandhuis.nl
Abstract: This document describes how IT suppliers can help the Dutch cultural heritage institutions to become more compliant with the principles for cooperation in the Dutch Digital Heritage Network (NDE).
</pre>

Introduction {#intro}
=====================

This document describes how IT suppliers can help the Dutch cultural heritage institutions to become more compliant with the principles
for cooperation in the Dutch Digital Heritage Network (NDE). It is and will be constructed in full cooperation with suppliers.
We ask IT suppliers of data management and publication software to implement the guidelines outlined in this document in their products.
This will help the Dutch heritage institutions in making their information accessible to machines and humans on the web.

The goal of cooperation in the network is to increase the public value of the cultural heritage information by improving its visibility,
usability and sustainability. A crucial part of this challenge is to improve the capabilities of institutions to share their information
in the network and to integrate digital heritage information from various sources to build services for their audiences.

This document aims to initiate an open discussion and cooperation with the technical partners in the network. The final goal is to document
practical solutions that are within reach of the various IT systems used by the institutions and developed by the IT suppliers actively
cooperating with NDE. Where appropriate, formal specifications of these solutions will be proposed to the Architecture Board of the DERA
to become part of the long term agreements within the cultural heritage domain.

Relation with other documentation
--------------------

This document builds upon the [high-level functional design](https://github.com/netwerk-digitaal-erfgoed/high-level-design) for the
network of cultural heritage information created earlier in the NDE program. In the high-level design, we define terms, building blocks and functions.
The design in itself is based on the [Dutch digital heritage reference architecture (DERA)](https://dera.netwerkdigitaalerfgoed.nl/),
the [National Digital Heritage Strategy](https://www.netwerkdigitaalerfgoed.nl/en/about-us/national-digital-heritage-strategy/) and
the [NDE position paper](https://www.netwerkdigitaalerfgoed.nl/wp-content/uploads/2018/02/NDE_PositionPaper_NetworkHeritageInformation-EN-v2.pdf).

Overview
--------------------

The guidelines in this document follow the general practice for publishing [Linked Data](https://en.wikipedia.org/wiki/Linked_data).
As stated in the [reference architecture](https://dera.netwerkdigitaalerfgoed.nl/index.php/Linked_Data) (in Dutch) Linked Data is regarded as a
key technology in the cultural heritage domain for integrating the huge amount of information resources within the network. Linked Data is
the driver for providing and maintaining coherent digital services within the network.

The information resources should be self-contained in respect to persistent identifiers
(see [section 5](#adding-persistent-identifiers),
'[Uniform Resource Identifiers](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier)', URIs) and metadata
(see [section 2](#publishing-collection-information)).
This metadata should contain as many terms as possible (see [section 3](#connecting-sources)). Information resources can be combined and published in datasets. Each dataset 
should be described as a separate resource with a unique identifier and additional metadata describing the characteristics
of the dataset (see [section 4](#publishing-dataset-descriptions)).

Quick access through questions
--------------------

* [Which data models are recommended?](#data-model)
* [Which RDF-serialization is recommended?](#rdf-serialization)
* [Which technique is recommended to provide data through the web?](#publishing-linked-data)
* [How can I use an internal terminology source?](#using-your-own-terminology-source)
* [How can I use an external terminology source?](#using-standardized-terminology-sources)
* [Which processes and data models are recommended for publishing terminology sources?](#publishing-terminology-sources)
* [Which data models are recommended for publishing dataset information?](#publication-model-for-dataset-descriptions)
* [Which techniques are recommended to provide datasetdescriptions through the web?](#how-to-publish-dataset-descriptions)
* [How do I promote my datasets?](#spreading-the-knowledge-about-datasets)
* How are persistent identifiers stored in the database?
* Which protocols are recommended for persistent identification?

About this document
--------------------

RDF can be published in various serializations. For readability, we present examples in Turtle serialization,
although we recommend other serializations in different situations.

Publishing collection information {#publishing-collection-information}
=====================

Publishing information resources as Linked Data can be done by transforming the internal data of the collection management system
to a Linked Data format as described by the [Resource Description Framework](https://www.w3.org/TR/rdf11-concepts/) (RDF).
The primary focus of the guidelines in this document is to improve the visibility and interoperability of the heritage collections.
These guidelines describe how to create interoperability with Linked Data and do not prescribe which techniques must be used to store the data,
let alone technology within the systems.

Learn the basics of Linked Data
* [in this book](http://linkeddatabook.com/editions/1.0/)
* [in this online course](https://open.hpi.de/courses/semanticweb2016)

Data model {#data-model}
--------------------

### Generic data model

The main goal for publishing the object descriptions as Linked Data is to improve the data integration and visibility in the network.
Because the heritage network spans institutions from the library, archive and museum domains, a generic data model that can support
general visibility on the web is needed. In order to support broader visibility on the web outside the cultural heritage domain,
data should be published with [Schema.org](https://schema.org/). Schema.org is a joint initiative of three major search engines,
Bing, Google and Yahoo, with the aim of setting up a shared scheme to structure data. Vocabularies from Schema.org are developed through an open community process.

Interesting links:
* [https://www.contentking.nl/academy/schema-structured-data/](https://www.contentking.nl/academy/schema-structured-data/) (in Dutch)

Thanks to Europeana, most institutions and collection management systems are familiar with Dublin Core (DC). Based on DC, a generic data model
for cultural heritage institutions has been defined, the Europeana Data Model (EDM). [Recent investigations](https://github.com/netwerk-digitaal-erfgoed/lod-aggregator)
have shown that well-formed Schema.org data can be transformed to EDM without significant loss of detail. Schema.org contains properties
that are very similar to Dublin Core properties. A complete mapping between Schema.org and Dublin Core can be found [here](https://dcmi.github.io/schema.org/).

Interesting links:
* [https://seopressor.com/blog/dublin-core-vs-schemaorg-metadata-comparison/](https://seopressor.com/blog/dublin-core-vs-schemaorg-metadata-comparison/)

Some basic examples for:
* a [photo with Dublin Core properties](example-2.ttl)
* the same [photo with Schema.org properties](example-3.ttl)
* a [museum object with Schema.org properties](example-4.ttl)
* a [two museum objects with Schema.org properties](example-5.ttl), showing different rdf:types

### Domain data model

The generic model described above will improve the general visibility of the cultural heritage objects, the terms describing the objects
and the datasets in which the objects and terms are grouped. In many cases, this will lead to further exploration of these objects.
For specific users or communities, a deeper understanding of the knowledge in the data will be important. Because of the open structure
of Linked Data, there is no restriction to simply add another layer of description to the resource. See for [example](example-1.ttl)
this object description. So institutions can add extra properties and class definitions to the Linked Data they publish.
In general \[CIDOC-CRM](http://www.cidoc-crm.org/) (and its derivative [Linked Art](https://linked.art/model/)) can be appropriate for museums.
For archives, the [RiC-O ontology](https://www.ica.org/standards/RiC/ontology) seems to be promising and for
libraries, [RDA Elements](https://www.rdaregistry.info/Elements/) or \[BIBFRAME](https://www.loc.gov/bibframe/) could be relevant.
It is up to the institutions in the separate domains to agree on the use and implementation of these shared standards.
Best practices and information about actual implementations will be shared in the network.

See [Awesome Humanities Ontologies](https://github.com/CLARIAH/awesome-humanities-ontologies) for more options.

### Rights

The DERA requires institutions to publish their metadata with an open license (for some institutions this requirement might still be a challenge).
The actual access to the digital object, such as an image or audio file, can be restricted; additional license statements for use and reuse of the objects must be specified in the metadata.

TBD:
* use schema:license
* differentiate license for
	* dataset
	* object metadata
	* metadata reproduction

RDF serialization {#rdf-serialization}
--------------------

RDF - the framework to express data in triples - can be formatted in various serializations. Different serializations are useful in different situations.

### \'Traditional' linked data

The oldest type of serialization is RDF/XML. This format is complex and old-fashioned, but it could be very useful in XML ecosystems.
The best readable serialization is Turtle. Examples in this document are written in this form. Linked data provided
in '[N-Triples](https://nl.wikipedia.org/wiki/N-Triples)' format can be processed easily in all linked data tooling.

Interesting link:
* [https://ontola.io/blog/rdf-serialization-formats/](https://ontola.io/blog/rdf-serialization-formats/)

In almost all programming languages libraries are available to transform one RDF serialization into another. Check for libraries in your favourite programming language.

Please offer your data in at least N-Triples format. See for example [this photo example](nt-examples/example-3.nt) that we introduced in Turtle format earlier.

If triples are published in both the generic and the domain model they are all included in the same data file.

You could provide more than one serialization and offer the user a choice with 'content negotiation'. (see [section 2.3.2](#web-compliancy-level-resolvable-uris)).

### JSON-LD

JSON-LD is the youngest format and comprehensible to web developers that are used to JSON. The format is more difficult to process by Linked Data applications. We recommend only using JSON-LD inline in the landing page of an information resource.
 (see [section 2.3.2](#web-compliancy-level-resolvable-uris))

Publishing Linked Data {#publishing-linked-data}
--------------------

Linked Data can be published in several ways. In this section, we will discuss four methods ranging in levels of complexity of implementation.
User applications could prefer one of these methods to obtain the data for their purpose, so the publication methods should be offered in parallel.

### Basic level: Data dumps
Although not a proper Linked Data implementation, publishing a file in RDF-format supports basic data services. This section describes requirements for such a data dump.

Most of the current collection management systems support XML-based export formats. In many cases, a conversion using XSLT Stylesheets is
possible to generate a Linked Data dump. The system should support a workflow that can be operated periodically by the system administrator
in order to produce the dump. The dump should be described by a dataset description. The link pointing to the dump should be fit for
automatic processing and therefore be accessible by HTTP clients, like curl.

A dump could be either a zip file containing files for every information resource or one big file containing all the triples. NDE prefers a data dump
in one file containing N-Triples because files with N-Triples from various sources can be easily combined and easily cut into pieces for processing.

In some cases, conversion cannot be done by the collection management system itself and a separate publication functionality is required.
This can be another system within the institution or a system run by a partner in the network (an aggregator or other service provider).
Export in CSV format could for instance be converted into RDF with the [LDWizard](https://github.com/netwerk-digitaal-erfgoed/LDWizard-ErfgoedWizard).

### Web compliance level: resolvable URIs ### {#web-compliancy-level-resolvable-uris}

Although dumps are sufficient to support basic services, proper Linked Data implementations should provide Linked Data per object,
in Linked Data terminology called a ‘resource’. Each resource in the collection should have a stable and well-formed URI.
This URI should be ‘resolvable’. This means that the web server provides data about the resource in a format depending on the Accept header
in the HTTP request. This functionality is called [content negotiation](https://www.w3.org/Protocols/rfc2616/rfc2616-sec12.html) which enables
browsers or other applications to ask for a specific language or representation of the available content. For Linked Data this mechanism is
used to differentiate between human-readable content (in HTML) and machine-readable content (in RDF).

For building real-time services that integrate disparate resources, content negotiation should be in place to serve the Linked Data.
In this case, creating a dump is no longer sufficient as it is possible to describe the dataset as a collection of resolvable URIs.

To improve the overall discoverability on the web it is recommended to add a representation of the data of the information resource with concepts and properties in Schema.org to the landing page of the object on the website. [Google recommends](https://developers.google.com/search/docs/guides/intro-structured-data) doing this by including
the Linked Data as \[JSON-LD](https://json-ld.org/) in the page. The JSON-LD could be provided separately from the webpage and included
with a JavaScript function. See this [example](http://cclod.netwerkdigitaalerfgoed.nl/test.html).

### Advanced level: queryable Linked Data ### {#advanced-level-queryable-linked-data}

Instead of building large centralized services, NDE envisions a distributed network of empowered organizations that share and reuse their
digital information directly. This means that organizations, data providers or service providers, can easily query and combine data from
several sources in the network. This satisfies two needs.

Firstly, it enables users to create a dedicated query for selecting and harvesting specific data. This data could be combined in a
thematic data service, e.g. [Netwerk Oorlogsbronnen](https://www.oorlogsbronnen.nl/) or [Van Gogh Worldwide](https://vangoghworldwide.org/). The facility could be used as a more sophisticated endpoint, in addition to an
OAI-PMH endpoint or a custom API.

Secondly, this kind of querying could be used dynamically: whenever a user wants to combine data, various endpoints could be queried and
results could be combined on the fly. This way of combining data, although a technological challenge, is already feasible in small-scale applications.

The first step in this direction is to make queryable Linked Data available. This can be done by implementing a SPARQL endpoint. Another, less demanding technology is the use of
[Triple Pattern Fragments](https://linkeddatafragments.org/specification/triple-pattern-fragments/). This offers an easy way of querying data and is much more scalable but puts a larger burden on the client for processing the results.

Further exploration in the area of decentralized querying has to take place and NDE invites the IT suppliers to work together on example
implementations to determine what practical steps can be made.

### Other considerations for Linked Data publication

Linked Data in the core of the collection management systems is not an explicit requirement for NDE. In the future, it will become more important to
reorganize the internal data structure of the systems so that it aligns with the Linked Data principles. This will make the transformation to
Linked Data more straightforward and will offer more opportunities to profit from the reuse of external Linked Data. NDE welcomes cooperation
with IT suppliers considering moving in this direction.

Another area that needs further exploration is the handling of multiple layers of object descriptions, for example originating
from different sources of other institutions, digital humanities research projects or crowdsourcing projects. Keeping track of the provenance
of different versions or the ability to maintain and publish different (time) versions of the same resource descriptions
(e.g. with [Memento](http://timetravel.mementoweb.org/about/), [Web Annotations](https://www.w3.org/TR/annotation-model/)) are other areas where
exploration needs to be done. For these topics NDE seeks cooperation with IT suppliers and with the [CLARIAH-PLUS project](https://www.clariah.nl/),
responsible for building research infrastructures in the Digital Humanities domain.

A third development is the concept of ['profile negotiation'](https://www.w3.org/TR/dx-prof-conneg/). In addition to content negotiation, profile negotiation enables a user to request data according to a certain data model. In our case the user could be able to choose between the generic data model or the domain data model.

Connecting sources {#connecting-sources}
=====================

"Connected sources" are sources that refer to each other and use each other's information. This is done using terms.
Terms are, for example, subjects, persons or places. Each term has a URI as an identifier. Because a URI is a unique web address
the URI makes unambiguously clear which term is meant. Terms and their URIs are managed in terminology sources, such as thesauri,
reference lists or classification systems. Examples are \[AAT](https://www.getty.edu/research/tools/vocabularies/aat/),
[GeoNames](https://www.geonames.org/) or [WO2 Thesaurus](https://data.niod.nl/WO2_Thesaurus.html) (Dutch).

For connecting sources, it is important that institutions relate URIs of terms to the descriptions of their heritage objects.
There are various options for this which are discussed below. The options are increasing in complexity – for example, they
require changes to the collection management system – but also provide better functionality.

Using your own terminology source {#using-your-own-terminology-source}
--------------------

An institution can create and use its own terminology source. Most of the time a collection management system facilitates the
development and maintenance of a thesaurus or set of persons. These sources contain the terms that the collection manager can use
when describing heritage objects. This makes it easy to relate terms. A limitation, however, is that the maintenance of one's own
source could be cumbersome. Another limitation is that the terms in the source may not be known to users outside the institution.
This reduces the ability to connect sources from different institutions.

In the terminology management module of a collection management system, the user should be able to relate the internal term to an
external one. One could for instance state that the internal term for \'schilderij' is an exact match with the external term \'paintings' in
the AAT. Other types of relations are possible, for instance, an internal term \'oil-painting' is a narrower term of the AAT \'paintings'
(see: [section 3.3.2](#technology-and-datamodel))
Of course this external term is defined with the proper URI and stored as such. The process of relating terms from one (internal)
terminology source with another (external) one, is called *alignment*.

If a terminology source is developed and used instead of a standardized one, the terminology source apparently has an added value to
the field. For that reason, the collection management system must be able to publish the terminology source, with alignments, as
Linked Data (see: [section 3.3](#publishing-terminology-sources)).

Using standardized terminology sources {#using-standardized-terminology-sources}
--------------------

Using terms that are not connected to the network of internationally standardized terminology sources reduces the ability to connect sources
from different institutions. We recommend using as many standardized terms as possible.

### Manually searching a terminology source ### {#manually-searching-a-terminology-source}

This is the simplest, low-tech approach to assign the URI of a term to the description of a heritage object. A collection manager goes
to the online system of a terminology source and searches for a term, for example, \'paintings' in AAT. The manager then copies the URI of the
term, goes back to her collection management system and pastes the URI in a designated field. This approach requires few changes to the
collection management system: a field has to be created in order to store the URI. On the other hand, this approach is cumbersome:
a manager has to perform some actions manually.

### Adding terms in bulk

Instead of adding a (URI of a) term to an object description one-by-one, the curator could export the data from her system and add the
terms and their URIs in bulk. This could be done with applications like [Open Refine](https://openrefine.org/). For this approach, the
collection management system must be able to export the data, for instance in CSV format, and import the new data with its additional
link to the object.

The addition of a term can be based on an existing field containing only a string of the data, for instance \'schilderij'. By using
search-and-replace kind of tricks the additional link could be added. This type of work is called *reconciliation*.

### Importing a terminology source

An institution can import one or more terminology sources into its collection management system, such as AAT and/or WW2 Thesaurus.
This makes it easy to use terms: the collection manager can search for terms in her management system, without having to switch to the
system of the source, as is the case when searching terms manually
(see [section 3.2.1](#manually-searching-a-terminology-source)).
A limitation is that the manager does not use current data: an import takes place periodically – for example: once a day or month – so that
changes to the terminology source are not immediately visible.

### Searching a terminology source in real-time

An institution can query a terminology source in real-time in its collection management system. A collection manager first enters a search query
in a search field. Next, her management system – in the background – queries the terminology source, retrieves the terms that correspond to the
query and presents them to the manager. The manager then chooses the desired term and the management system stores the URI of the chosen term.
This ensures that the institution works with current data. However, the implementation of this approach in the management system can entail
some difficulties: a connection must be established with the source – or multiple connections if the institution wants to use multiple sources.

### Using the Network of Terms

An institution can use the [Network of Terms](https://termennetwerk.netwerkdigitaalerfgoed.nl/faq). The Network of Terms is a search engine
for terms: it searches one or more terminology sources in real-time. The Network of Terms can be used in a collection management system.
A collection manager first enters a search query in a search field. Next, her management system – in the background – queries the Network of Terms,
retrieves the terms that correspond to the query and presents them to the manager. The manager then chooses the desired term and
the management system stores the URI of the chosen term. This makes it easier to use multiple terminology sources: you only need to connect
to the Network of Terms.

### Indexing data from the terminology source

If (the URI of) a term is stored, collection managers might be interested in related data in the collection management system, for instance,
alternative labels, pseudonyms, or labels in different languages. This helps the user of the collection management system to retrieve data
from her system. As long as the web of data is not as decentralized as we'd wish
(see [section 2.3.3](#advanced-level-queryable-linked-data)),
collection management systems could offer to obtain the needed data via resolving the URI
(see [section 2.3.2](#web-compliancy-level-resolvable-uris))
and store and index the extra data in the database. The related data must be synchronized periodically and the collection management interface
must enable to force the synchronization. A collection manager that helps the editors of the standardized source to add or improve data,
can then see her results quickly.

Publishing terminology sources {#publishing-terminology-sources}
--------------------

"Published terminology sources" are sources that are important to the cooperating institutions in the Dutch Digital Heritage Network.
It enables collection managers to find and use terms in the sources more easily when describing their heritage objects. And that makes
digital heritage found easier by data-users. Publishing terminology sources should obey
[these requirements](https://netwerk-digitaal-erfgoed.github.io/requirements-terminologiebronnen/) (Dutch).

A terminology source is considered a dataset, with sustainable URIs as described
in [section 5](#adding-persistent-identifiers), described according
to the guidelines in [section 4](#publishing-dataset-descriptions),
and published as dump, with resolvable URIs and in a SPARQL query as described in
[section 2](#publishing-collection-information).

A terminology source can only be used if it meets a few conditions. Are you the manager of a source and are you considering making it
available to the network? Then take the following aspects into account.

### Authority

Terms are not just ordinary words: they are official notations. The manager of a terminology source guarantees their quality.
For example: terms must be accurate and current. Another example: the manager ensures that her terms are permanently available and can
be used for a long time. This makes a terminology source an authority on which other institutions can build.

### Technology and data model ### {#technology-and-datamodel}

A terminology source is used by both humans and machines. For example, institutions' collection management systems must be able to connect
to the source. To make this possible, the source must meet certain technological requirements. These requirements follow the
Linked Data principles and best practices. This means, for example, that every term in the source gets a sustainable URI; when dereferencing
this URI, metadata about the term is returned. This metadata must be expressed according to a specific RDF metadata model,
for example, [Simple Knowledge Organization System (SKOS)](https://www.w3.org/2009/08/skos-reference/skos.html) or Schema.org.
SKOS contains various types of relations, eg. skos:exactMatch, skos:closeMatch or skos:broader to store an alignment between your
terminology source and standardized terms.

In addition, terms in the source must be searchable through a SPARQL endpoint. This enables a connection to the Network of Terms.

### Cooperation

A terminology source is used by institutions. There is a good chance that these institutions do not use one, but several sources.
In order to increase the ease of use of sources, the collaboration between source managers and collection managers is essential.
For example about the way in which collection managers can propose changes to terms to sources: this could follow a uniform process.
Another example: it can happen that two sources define the same term (such as \'painting'). The sources should then jointly agree
that these terms have the same meaning and refer to each other's terms – this clarifies the relationship between both terms for collection managers.
The collection management system should support these processes.

Publishing dataset descriptions {#publishing-dataset-descriptions}
=====================

To increase the findability of datasets of heritage institutions, it is important to publish the dataset descriptions according to a
well documented, machine-readable model. When rich dataset descriptions are used, published not only as HTML (for humans) but also as
meaningful metadata (for the machine), the findability and use of datasets that heritage institutions make available, will improve.
Publishing dataset descriptions should obey [these requirements](https://netwerk-digitaal-erfgoed.github.io/requirements-datasets/),
which contains a more elaborate description of the publishing process.

Publication model for dataset descriptions {#publication-model-for-dataset-descriptions}
--------------------

The NDE has drafted a publication model which is based on Schema.org. For datasets, the class https://schema.org/Dataset has been developed.
This class is based on W3C DCAT work and benefits from collaboration around the DCAT, ADMS and VoID vocabularies.

<!--TBD See [https://github.com/netwerk-digitaal-erfgoed/project-organisations-datasets/tree/master/publication-model](https://github.com/netwerk-digitaal-erfgoed/project-organisations-datasets/tree/master/publication-model) for the the complete specification of the publication model.-->

How to publish dataset descriptions {#how-to-publish-dataset-descriptions}
--------------------

For good findability, every dataset description must be accessible via the Internet, must be legible for humans and machines, and use the
publication model. The serialization of dataset descriptions can take several serializations, JSON-LD is preferred.

A user can obtain a dataset description with a sustainable URI as described in
[section 5](#adding-persistent-identifiers) and published as a
resolvable URI as described in [section 2.3.2](#web-compliancy-level-resolvable-uris).

Most (automated) users expect to find the metadata in the page itself (inline). Spiders of search engines such as Google might not follow
linked JSON-LD files. Even if the linked files (via Javascript) are "injected" into the page, most spiders do not pick up on this.
There are more serializations of RDF, such as RDF/XML and Turtle. Spiders of search engines such as Google currently only support microdata,
RDFa and JSON-LD. Because search engine findability is an important driver, the use of inline JSON-LD is recommended. However,
this does not prevent the additional publication of the dataset description in other serialization formats or a (content negotiation based)
separate resource.

Spreading the knowledge about datasets {#spreading-the-knowledge-about-datasets}
--------------------

The choice for Schema.org as a basis for the publication model for dataset descriptions comes with the benefit that this model is
known (and used) by big search engines. By adding the dataset description pages to the sitemap of the website, search engines are likely
to pick up the dataset description and make them available in tools like [Google Dataset Search](https://datasetsearch.research.google.com/).

By registering the datasets with the NDE Register, the network of Dutch heritage institutions can easily find relevant datasets to link with.
More generic (open) dataset registers, like data.overheid.nl, can also be used to promote the datasets of the institutions.

Adding persistent identifiers {#adding-persistent-identifiers}
=====================

TBD
Everything (eg. heritage objects, terms, datasets) needs a persistent URI, unique in the world.

Persistent Identifiers
--------------------

* Please inform yourselves about 'persistent identifiers' on [this website](https://www.pidwijzer.nl/en).
* Handle

Uniform Resource Identifiers
--------------------

URIs are webaddresses. URI-strategies:
* [CONCEPT Nationale URI-Strategie voor Linked Data van de Nederlandse overheid](https://www.pldn.nl/wiki/Concept_URI-strategie) (dutch)
* [URI-beleid voor Linked Data [van de Koninklijke Bibliotheek](doc/URI-beleidLinkedData_1.1.2.pdf) (dutch)
* [Duurzaam vindbaar op het web: URI-strategie Nationaal Archief; het gebruik van Handles en URI’s](doc/URI-strategie-Nationaal-Archief_v1_1_INTERN.pdf) (dutch)
* [IISG URI Strategy](https://confluence.socialhistoryservices.org/display/DOC/IISG+URI+Strategy)
